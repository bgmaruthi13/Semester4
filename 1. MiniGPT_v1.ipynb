{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","authorship_tag":"ABX9TyMF81IC3fsuPNHBTNMyFVeO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["# ================================\n","# Mini-GPT for Text Generation\n","# ================================\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import random\n","\n","# ------------------\n","# 1️⃣ Set random seeds\n","# ------------------\n","torch.manual_seed(42)\n","np.random.seed(42)\n","random.seed(42)\n","\n","# ------------------\n","# 2️⃣ Sample dataset\n","# ------------------\n","text = (\n","    \"Once upon a time, in a land far away, there was a small village. \"\n","    \"In this village, lived a young girl named Alice who loved adventures. \"\n","    \"Every day, she would explore the forest, discovering new secrets and magical creatures. \"\n","    \"One day, she stumbled upon a mysterious door hidden behind the trees.\"\n",")\n","\n","# ------------------\n","# 3️⃣ Tokenization\n","# ------------------\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","stoi = {ch: i for i, ch in enumerate(chars)}\n","itos = {i: ch for ch, i in stoi.items()}\n","\n","def encode(s):  # string to integer sequence\n","    return [stoi[c] for c in s]\n","\n","def decode(l):  # integer sequence to string\n","    return \"\".join([itos[i] for i in l])\n","\n","data = torch.tensor(encode(text), dtype=torch.long)\n","\n","# Train/validation split\n","n = int(0.9 * len(data))\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","# ------------------\n","# 4️⃣ Hyperparameters\n","# ------------------\n","block_size = 8      # context size\n","batch_size = 4\n","embedding_dim = 32\n","n_heads = 4\n","n_layers = 2\n","dropout = 0.1\n","lr = 1e-2\n","epochs = 1000\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# ------------------\n","# 5️⃣ Data loader\n","# ------------------\n","def get_batch(data, batch_size, block_size):\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    return x.to(device), y.to(device)\n","\n","# ------------------\n","# 6️⃣ Mini-GPT Model\n","# ------------------\n","class Head(nn.Module):\n","    \"\"\"One attention head\"\"\"\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n","        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n","        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n","        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n","\n","    def forward(self, x):\n","        B, T, C = x.shape\n","        k = self.key(x)   # (B,T,head_size)\n","        q = self.query(x) # (B,T,head_size)\n","        wei = q @ k.transpose(-2, -1) * C**-0.5\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n","        wei = F.softmax(wei, dim=-1)\n","        v = self.value(x)\n","        out = wei @ v\n","        return out\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(num_heads*head_size, embedding_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, embedding_dim):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(embedding_dim, 4*embedding_dim),\n","            nn.ReLU(),\n","            nn.Linear(4*embedding_dim, embedding_dim),\n","            nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Block(nn.Module):\n","    \"\"\"Transformer block: attention + feedforward\"\"\"\n","    def __init__(self, embedding_dim, n_heads):\n","        super().__init__()\n","        head_size = embedding_dim // n_heads\n","        self.sa = MultiHeadAttention(n_heads, head_size)\n","        self.ffwd = FeedForward(embedding_dim)\n","        self.ln1 = nn.LayerNorm(embedding_dim)\n","        self.ln2 = nn.LayerNorm(embedding_dim)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n","\n","class MiniGPT(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n","        self.position_embedding_table = nn.Embedding(block_size, embedding_dim)\n","        self.blocks = nn.Sequential(*[Block(embedding_dim, n_heads) for _ in range(n_layers)])\n","        self.ln_f = nn.LayerNorm(embedding_dim)\n","        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","        tok_emb = self.token_embedding_table(idx)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n","        x = tok_emb + pos_emb\n","        x = self.blocks(x)\n","        x = self.ln_f(x)\n","        logits = self.lm_head(x)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        for _ in range(max_new_tokens):\n","            idx_cond = idx[:, -block_size:]\n","            logits, _ = self(idx_cond)\n","            logits = logits[:, -1, :]\n","            probs = F.softmax(logits, dim=-1)\n","            next_idx = torch.multinomial(probs, num_samples=1)\n","            idx = torch.cat((idx, next_idx), dim=1)\n","        return idx\n","\n","model = MiniGPT().to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","\n","# ------------------\n","# 7️⃣ Training loop\n","# ------------------\n","for epoch in range(epochs):\n","    xb, yb = get_batch(train_data, batch_size, block_size)\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if epoch % 50 == 0:\n","        print(f\"Epoch {epoch}, Loss {loss.item():.4f}\")\n","\n","# ------------------\n","# 8️⃣ Generate text\n","# ------------------\n","context = torch.tensor(encode(\"Alice\"), dtype=torch.long, device=device).unsqueeze(0)\n","generated_idx = model.generate(context, max_new_tokens=50)[0].tolist()\n","print(\"Generated Text:\\n\", decode(generated_idx))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mkJ3vRcckgQP","executionInfo":{"status":"ok","timestamp":1756567886680,"user_tz":-330,"elapsed":33259,"user":{"displayName":"Maruthi Naman","userId":"07103931332797670520"}},"outputId":"0e9cf808-b759-4d0b-cd10-252de2d4700a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss 3.5432\n","Epoch 50, Loss 2.2185\n","Epoch 100, Loss 1.6028\n","Epoch 150, Loss 1.5447\n","Epoch 200, Loss 1.8483\n","Epoch 250, Loss 1.4301\n","Epoch 300, Loss 0.8368\n","Epoch 350, Loss 1.2287\n","Epoch 400, Loss 1.2923\n","Epoch 450, Loss 0.9025\n","Epoch 500, Loss 1.0508\n","Epoch 550, Loss 0.7236\n","Epoch 600, Loss 1.0718\n","Epoch 650, Loss 0.4333\n","Epoch 700, Loss 0.7527\n","Epoch 750, Loss 0.6715\n","Epoch 800, Loss 0.7509\n","Epoch 850, Loss 0.8164\n","Epoch 900, Loss 0.6230\n","Epoch 950, Loss 0.4751\n","Epoch 1000, Loss 0.5099\n","Epoch 1050, Loss 0.9172\n","Epoch 1100, Loss 0.4152\n","Epoch 1150, Loss 0.6441\n","Epoch 1200, Loss 0.5952\n","Epoch 1250, Loss 0.5502\n","Epoch 1300, Loss 0.5558\n","Epoch 1350, Loss 0.4457\n","Epoch 1400, Loss 0.5304\n","Epoch 1450, Loss 0.5324\n","Epoch 1500, Loss 0.5823\n","Epoch 1550, Loss 0.5969\n","Epoch 1600, Loss 0.7007\n","Epoch 1650, Loss 0.4960\n","Epoch 1700, Loss 0.4824\n","Epoch 1750, Loss 0.6776\n","Epoch 1800, Loss 0.4685\n","Epoch 1850, Loss 0.2972\n","Epoch 1900, Loss 0.4093\n","Epoch 1950, Loss 0.5261\n","Epoch 2000, Loss 0.6645\n","Epoch 2050, Loss 0.3795\n","Epoch 2100, Loss 0.5960\n","Epoch 2150, Loss 0.4184\n","Epoch 2200, Loss 0.7982\n","Epoch 2250, Loss 0.4128\n","Epoch 2300, Loss 0.5372\n","Epoch 2350, Loss 0.6085\n","Epoch 2400, Loss 0.3445\n","Epoch 2450, Loss 0.4126\n","Epoch 2500, Loss 0.3935\n","Epoch 2550, Loss 0.5272\n","Epoch 2600, Loss 0.6933\n","Epoch 2650, Loss 0.4576\n","Epoch 2700, Loss 0.8113\n","Epoch 2750, Loss 0.6550\n","Epoch 2800, Loss 0.5224\n","Epoch 2850, Loss 0.5607\n","Epoch 2900, Loss 0.5034\n","Epoch 2950, Loss 0.5247\n","Generated Text:\n"," Alice who  st,here was small villagicage, listAlice upo\n"]}]}]}